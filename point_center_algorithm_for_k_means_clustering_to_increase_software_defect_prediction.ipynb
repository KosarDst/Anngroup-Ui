{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2yWdT9f_y5hq",
        "jqRxAry4ZMwJ"
      ],
      "mount_file_id": "1TI0PXblOJgAUqYmx1evTWnAjj_t_mwPk",
      "authorship_tag": "ABX9TyOjCpJ63tKQ8ukaBWmWQsJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KosarDst/Anngroup-Ui/blob/main/point_center_algorithm_for_k_means_clustering_to_increase_software_defect_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Calculate the point center algorithm to get ùëò value and point center\n"
      ],
      "metadata": {
        "id": "N5w4RK2bstCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/CM1.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/KC1.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/KC3.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/MC2.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/MW1.csv')\n",
        "df6 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC1.csv')\n",
        "df7 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC2.csv')\n",
        "df8 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC3.csv')\n",
        "df9 = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC4.csv')\n",
        "\n",
        "# combine dataframes into one\n",
        "# df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9], ignore_index=True)\n",
        "# df = df.drop(df.index[0])\n",
        "# df = df.replace(to_replace=r'^\\D*$', value=0, regex=True)\n",
        "# df = df.apply(pd.to_numeric)"
      ],
      "metadata": {
        "id": "pzPM1dcHmIlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple K-means"
      ],
      "metadata": {
        "id": "2yWdT9f_y5hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df =pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC2.csv')\n",
        "\n",
        "# Remove the 'id' and 'Defective' columns from the DataFrame\n",
        "X = df.drop(['Defective'], axis=1)\n",
        "\n",
        "# Fit the K-Means model with k=2\n",
        "kmeans = KMeans(n_clusters=2).fit(X)\n",
        "\n",
        "# Predict the cluster labels\n",
        "labels = kmeans.predict(X)\n",
        "\n",
        "# Convert 'N' and 'Y' labels to integers\n",
        "label_map = {'N': 0, 'Y': 1}\n",
        "y_true = df['Defective'].map(label_map)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_true, labels)\n",
        "\n",
        "# # Extract the True Negative, False Positive, False Negative, and True Positive values\n",
        "tn, fn, fp, tp = cm.ravel()\n",
        "\n",
        "# Print the results\n",
        "print('True Negative:', tn)\n",
        "print('False Positive:', fp)\n",
        "print('False Negative:', fn)\n",
        "print('True Positive:', tp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLztpBSI0m_l",
        "outputId": "c5bdc0a3-01cb-42ef-f8ef-a271785425c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Negative: 1465\n",
            "False Positive: 15\n",
            "False Negative: 12\n",
            "True Positive: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposed K-means"
      ],
      "metadata": {
        "id": "jqRxAry4ZMwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC2.csv')\n",
        "\n",
        "# Identify string columns\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "# Replace string values with numeric\n",
        "df[string_cols] = df[string_cols].replace(['N', 'Y'], [0,1])\n",
        "\n",
        "# Replace missing values with mean\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# Calculate point center\n",
        "means = df.mean()\n",
        "k = len(df['Defective'].unique())\n",
        "centroids = [means] * k\n",
        "\n",
        "# Perform k-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=k, init=centroids, n_init=1)\n",
        "clusters = kmeans.fit_predict(df)\n",
        "\n",
        "# Calculate error and Rand index\n",
        "confusion_matrix = pd.crosstab(df['Defective'], clusters)\n",
        "print(confusion_matrix)\n",
        "error = (confusion_matrix.iloc[0,1] + confusion_matrix.iloc[1,0]) / confusion_matrix.sum().sum()\n",
        "rand_index = (confusion_matrix.iloc[0,0] + confusion_matrix.iloc[1,1]) / confusion_matrix.sum().sum()\n",
        "\n",
        "print(f\"Error: {error}\")\n",
        "print(f\"Rand Index: {rand_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ab_a-hBuCQ7",
        "outputId": "f6b6f6ae-63a4-45ed-c9e9-6a7bc9ea5dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "col_0         0  1\n",
            "Defective         \n",
            "0          1470  7\n",
            "1            15  1\n",
            "Error: 0.014735432016075016\n",
            "Rand Index: 0.985264567983925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# point center algorithm"
      ],
      "metadata": {
        "id": "RFy63ow6vS7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Dataset_SDP/PC3.csv')\n",
        "data = data.replace(to_replace=r'^\\D*$', value=0, regex=True)\n",
        "# Calculate the mean and standard deviation for each attribute\n",
        "means = data.mean()\n",
        "stds = data.std()\n",
        "\n",
        "# Determine the center data point based on the maximum average of standard deviation\n",
        "center = means + stds\n",
        "center_point = center.idxmax()\n",
        "\n",
        "# Calculate the Euclidean distance of each data point from the center point\n",
        "distances = np.sqrt(((data - center) ** 2).sum(axis=1))\n",
        "\n",
        "# Select the data point with the maximum distance as the first candidate center point (c1)\n",
        "c1 = data.iloc[distances.idxmax()]\n",
        "\n",
        "# Select variables X and Y based on minimum average and maximum standard deviation\n",
        "min_mean = means.min()\n",
        "max_std = stds.max()\n",
        "X = means[means == min_mean].index[0]\n",
        "Y = stds[stds == max_std].index[0]\n",
        "\n",
        "# Calculate the Euclidean distance from c1 and select the point with maximum distance as the second candidate center point (c2)\n",
        "distances_X = np.sqrt(((data[X] - c1[X]) ** 2))\n",
        "distances_Y = np.sqrt(((data[Y] - c1[Y]) ** 2))\n",
        "distances = np.sqrt(distances_X ** 2 + distances_Y ** 2)\n",
        "c2 = data.iloc[distances.idxmax()]\n",
        "\n",
        "# Continue this process until the distance equals a previous distance, giving you the number of clusters k\n",
        "prev_distance = 0\n",
        "k = 2\n",
        "while True:\n",
        "    distances_X = np.sqrt(((data[X] - c1[X]) ** 2))\n",
        "    distances_Y = np.sqrt(((data[Y] - c1[Y]) ** 2))\n",
        "    distances_c1 = np.sqrt(distances_X ** 2 + distances_Y ** 2)\n",
        "    distances_X = np.sqrt(((data[X] - c2[X]) ** 2))\n",
        "    distances_Y = np.sqrt(((data[Y] - c2[Y]) ** 2))\n",
        "    distances_c2 = np.sqrt(distances_X ** 2 + distances_Y ** 2)\n",
        "    distances = pd.concat([distances_c1, distances_c2], axis=1).min(axis=1)\n",
        "    distance = distances.sum()\n",
        "    if distance == prev_distance:\n",
        "        break\n",
        "    prev_distance = distance\n",
        "    k += 1\n",
        "    new_c = data.iloc[distances.idxmax()]\n",
        "    c1 = (c1 * (k - 1) + new_c) / k\n",
        "\n",
        "# The candidate center points c1, c2, ... , ck are the initial centroids for K-Means clustering\n",
        "initial_centroids = pd.concat([c1, c2], axis=1).T.to_numpy()\n",
        "\n",
        "print(\"Center point:\", center_point)\n",
        "print(\"Number of clusters (k):\", k)\n",
        "print(\"Initial centroids:\")\n",
        "print(initial_centroids)"
      ],
      "metadata": {
        "id": "G5xjP-p4JVkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**proposed method / not completed**"
      ],
      "metadata": {
        "id": "j9_A1Qfdu3Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, adjusted_rand_score\n",
        "\n",
        "# Step 1: Initialize the K value and point center value\n",
        "k = 2\n",
        "point_center = 3\n",
        "\n",
        "# Step 2: Load the dataset and perform KMeans clustering\n",
        "data = pd.read_csv('CM1.csv')\n",
        "X = data.drop('Defective', axis=1)\n",
        "kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Step 3: Evaluate the KMeans clustering using the proposed algorithm\n",
        "y_kmeans = kmeans.labels_\n",
        "y_proposed = [1 if x >= point_center else 0 for x in kmeans.predict(X)]\n",
        "cm = confusion_matrix(data['Defective'], y_proposed)\n",
        "error_rate = 1 - accuracy_score(data['Defective'], y_proposed)\n",
        "rand_index = adjusted_rand_score(data['Defective'], y_proposed)\n",
        "\n",
        "# Print the results\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(\"Error Rate:\", error_rate)\n",
        "print(\"Rand Index:\", rand_index)"
      ],
      "metadata": {
        "id": "5fZccCntux19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}